{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Paperspace RunAssignment14 v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvnk/EVA/blob/master/Assignment14/Paperspace_RunAssignment14_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrC03E6iHD2t",
        "colab_type": "text"
      },
      "source": [
        "## Bhuvnesh Kumar  // Saturday 11am batch \n",
        "\n",
        "Ran on Paperspace \n",
        "* V100  \n",
        "* 30gb ram\n",
        "* 8vcpu\n",
        "\n",
        "* Acc: 94.04\n",
        "* time: 292 Sec\n",
        "* extra augmentation: cutout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKwBkmwwG_wI",
        "colab_type": "code",
        "colab": {},
        "outputId": "bb9578de-5aed-4672-e7c3-05382f2a1e79"
      },
      "source": [
        "import numpy as np\n",
        "import time, math\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5ngFX8G_wf",
        "colab_type": "code",
        "colab": {},
        "outputId": "d9b6e248-4914-4882-d850-645582b32dc1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 24 17:45:44 2019       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   38C    P0    36W / 300W |     80MiB / 16130MiB |      0%      Default |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                       GPU Memory |\r\n",
            "|  GPU       PID   Type   Process name                             Usage      |\r\n",
            "|=============================================================================|\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK5DNpufG_wt",
        "colab_type": "code",
        "colab": {},
        "outputId": "74e0927a-8e1e-45ed-902d-ad597f379000"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "multiprocessing.cpu_count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5kd28XIG_w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39E_ObPwG_xI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
        "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
        "LEARNING_RATE = 0.4 #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 5e-4 #@param {type:\"number\"}\n",
        "EPOCHS = 24 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuSNckL6G_xR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_pytorch(shape, dtype=tf.float16, partition_info=None):\n",
        "  fan = np.prod(shape[:-1])\n",
        "  bound = 1 / math.sqrt(fan)\n",
        "  return tf.random.uniform(shape, minval=-bound, maxval=bound, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFsg1pt-G_xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBN(tf.keras.Model):\n",
        "  def __init__(self, c_out):\n",
        "    super().__init__()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters=c_out, kernel_size=3, padding=\"SAME\", kernel_initializer=init_pytorch, use_bias=False)\n",
        "    self.bn = tf.keras.layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.nn.relu(self.bn(self.conv(inputs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZgtwMwYG_xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlk(tf.keras.Model):\n",
        "  def __init__(self, c_out, pool, res = False):\n",
        "    super().__init__()\n",
        "    self.conv_bn = ConvBN(c_out)\n",
        "    self.pool = pool\n",
        "    self.res = res\n",
        "    if self.res:\n",
        "      self.res1 = ConvBN(c_out)\n",
        "      self.res2 = ConvBN(c_out)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    h = self.pool(self.conv_bn(inputs))\n",
        "    if self.res:\n",
        "      h = h + self.res2(self.res1(h))\n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XktyQ0GCG_xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DavidNet(tf.keras.Model):\n",
        "  def __init__(self, c=64, weight=0.2):\n",
        "    super().__init__()\n",
        "    pool = tf.keras.layers.MaxPooling2D()\n",
        "    self.init_conv_bn = ConvBN(c)\n",
        "    self.blk1 = ResBlk(c*2, pool, res = True)\n",
        "    self.blk2 = ResBlk(c*4, pool)\n",
        "    self.blk3 = ResBlk(c*8, pool, res = True)\n",
        "    self.pool = tf.keras.layers.GlobalMaxPool2D()\n",
        "    self.linear = tf.keras.layers.Dense(10, kernel_initializer=init_pytorch, use_bias=False)\n",
        "    self.weight = weight\n",
        "\n",
        "  def call(self, x, y):\n",
        "    h = self.pool(self.blk3(self.blk2(self.blk1(self.init_conv_bn(x)))))\n",
        "    h = self.linear(h) * self.weight\n",
        "    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h, labels=y)\n",
        "    loss = tf.reduce_sum(ce)\n",
        "    correct = tf.reduce_sum(tf.cast(tf.math.equal(tf.argmax(h, axis = 1), y), tf.float16))\n",
        "    return loss, correct"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5alHlR0G_xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "len_train, len_test = len(x_train), len(x_test)\n",
        "\n",
        "num_classes = 10\n",
        "# y_train = tf.keras.utils.to_categorical(y_train, num_classes, dtype='float16')\n",
        "# y_test = tf.keras.utils.to_categorical(y_test, num_classes, dtype='float16')\n",
        "y_train = y_train.astype('int64').reshape(len_train)\n",
        "y_test = y_test.astype('int64').reshape(len_test)\n",
        "\n",
        "train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "train_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "normalize = lambda x: ((x - train_mean) / train_std).astype('float16') # todo: check here\n",
        "pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
        "\n",
        "x_train = normalize(pad4(x_train))\n",
        "x_test = normalize(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2frNJ1uG_x1",
        "colab_type": "text"
      },
      "source": [
        "# Cutout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11SE8lzrG_x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_slice(input_: tf.Tensor, replacement, begin) -> tf.Tensor:\n",
        "    inp_shape = tf.shape(input_)\n",
        "    size = tf.shape(replacement)\n",
        "    padding = tf.stack([begin, inp_shape - (begin + size)], axis=1)\n",
        "    replacement_pad = tf.pad(replacement, padding)\n",
        "#     replacement_pad = tf.cast(replacement_pad, dtype=tf.float16)\n",
        "    mask = tf.pad(tf.ones_like(replacement, dtype=tf.bool), padding)\n",
        "    return tf.where(mask, replacement_pad, input_)\n",
        "\n",
        "def cutout(x: tf.Tensor, h: int=8, w: int=8, c: int = 3) -> tf.Tensor:\n",
        "    shape = tf.shape(x)\n",
        "    x0 = tf.random.uniform([], 0, shape[0] + 1 - h, dtype=tf.int32)\n",
        "    y0 = tf.random.uniform([], 0, shape[1] + 1 - w, dtype=tf.int32)\n",
        "    \n",
        "    x = replace_slice(x, tf.zeros([h, w, c], dtype = tf.float16), [x0, y0, 0])\n",
        "#     x = replace_slice(x, tf.constant(1,shape = [h,w,c],dtype = tf.float16)*train_mean, [x0, y0, 0])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-dIL2yeG_x-",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvxBttcBG_yA",
        "colab_type": "text"
      },
      "source": [
        "# With\n",
        "lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, 21, EPOCHS], [0, LEARNING_RATE,0.021, 0])[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8_XT7gQG_yB",
        "colab_type": "code",
        "colab": {},
        "outputId": "7fc914fa-024b-407f-9a53-1a980258b47b"
      },
      "source": [
        "model = DavidNet()\n",
        "#model.summary()\n",
        "batches_per_epoch = len_train//BATCH_SIZE + 1\n",
        "# LEARNING_RATE=0.4\n",
        "lr_schedule = lambda t: np.interp([t], [0, (EPOCHS+1)//5, 21, EPOCHS], [0, LEARNING_RATE,0.021, 0])[0]\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
        "# opt = tf.train.MomentumOptimizer(lr_func, momentum=MOMENTUM, use_nesterov=True)\n",
        "data_aug = lambda x, y: (cutout(tf.image.random_flip_left_right(tf.random_crop(x, [32, 32, 3]))), y)\n",
        "\n",
        "\n",
        "t = time.time()\n",
        "test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
        "\n",
        "mom_state = {}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = test_loss = train_acc = test_acc = 0.0\n",
        "  train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(data_aug, num_parallel_calls=8).shuffle(len_train).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "  tf.keras.backend.set_learning_phase(1)\n",
        "#   for (x, y) in tqdm(train_set):\n",
        "  for (x, y) in train_set:\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss, correct = model(x, y)\n",
        "\n",
        "    var = model.trainable_variables\n",
        "    grads = tape.gradient(loss, var)\n",
        "    \n",
        "    global_step.assign_add(1)\n",
        "    lr = lr_func()\n",
        "\n",
        "    for g, v in zip(grads, var):\n",
        "      g += v * WEIGHT_DECAY * BATCH_SIZE\n",
        "      \n",
        "      if v not in mom_state:\n",
        "        state = tf.zeros_like(g)\n",
        "      else:\n",
        "        state = mom_state[v]\n",
        "        \n",
        "      state = state * MOMENTUM + g\n",
        "      g += state * MOMENTUM\n",
        "      v.assign_add(g * (-lr))\n",
        "      mom_state[v] = state\n",
        "\n",
        "#     opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
        "\n",
        "    train_loss += loss.numpy()\n",
        "    train_acc += correct.numpy()\n",
        "\n",
        "  tf.keras.backend.set_learning_phase(0)\n",
        "  for (x, y) in test_set:\n",
        "    loss, correct = model(x, y)\n",
        "    test_loss += loss.numpy()\n",
        "    test_acc += correct.numpy()\n",
        "    \n",
        "  print('epoch:', epoch+1, 'lr:', lr_schedule(epoch+1), 'train loss:', train_loss / len_train, 'train acc:', train_acc / len_train, 'val loss:', test_loss / len_test, 'val acc:', test_acc / len_test, 'time:', time.time() - t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0824 17:45:55.403690 140525221713728 deprecation.py:323] From <ipython-input-12-c9dc21f47e08>:8: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 lr: 0.08 train loss: 1.538935 train acc: 0.44014 val loss: 1.2504 val acc: 0.5856 time: 17.21765923500061\n",
            "epoch: 2 lr: 0.16 train loss: 0.99907 train acc: 0.64874 val loss: 1.056825 val acc: 0.6439 time: 29.396451234817505\n",
            "epoch: 3 lr: 0.24 train loss: 0.793065 train acc: 0.72694 val loss: 1.076775 val acc: 0.6747 time: 41.439059257507324\n",
            "epoch: 4 lr: 0.32 train loss: 0.659765 train acc: 0.77282 val loss: 0.900975 val acc: 0.7148 time: 53.11166858673096\n",
            "epoch: 5 lr: 0.4 train loss: 0.576995 train acc: 0.79802 val loss: 0.7360625 val acc: 0.7521 time: 64.86671018600464\n",
            "epoch: 6 lr: 0.37631250000000005 train loss: 0.500575 train acc: 0.82768 val loss: 0.55155 val acc: 0.8086 time: 76.6717689037323\n",
            "epoch: 7 lr: 0.352625 train loss: 0.44165 train acc: 0.84704 val loss: 0.6141125 val acc: 0.7959 time: 88.26883912086487\n",
            "epoch: 8 lr: 0.3289375 train loss: 0.4107225 train acc: 0.85878 val loss: 0.5123 val acc: 0.8252 time: 100.2941381931305\n",
            "epoch: 9 lr: 0.30525 train loss: 0.37867 train acc: 0.86958 val loss: 0.4385625 val acc: 0.852 time: 112.05120992660522\n",
            "epoch: 10 lr: 0.28156250000000005 train loss: 0.35942 train acc: 0.87542 val loss: 0.61345 val acc: 0.7917 time: 123.67095065116882\n",
            "epoch: 11 lr: 0.257875 train loss: 0.34045375 train acc: 0.8842 val loss: 0.3997875 val acc: 0.863 time: 135.4792079925537\n",
            "epoch: 12 lr: 0.23418750000000002 train loss: 0.31103625 train acc: 0.89316 val loss: 0.40645 val acc: 0.8658 time: 147.16503024101257\n",
            "epoch: 13 lr: 0.21050000000000002 train loss: 0.29061125 train acc: 0.90006 val loss: 0.3744 val acc: 0.8739 time: 159.04096722602844\n",
            "epoch: 14 lr: 0.18681250000000002 train loss: 0.272395 train acc: 0.90574 val loss: 0.39021875 val acc: 0.8681 time: 170.90336322784424\n",
            "epoch: 15 lr: 0.16312500000000002 train loss: 0.25132 train acc: 0.91186 val loss: 0.4872375 val acc: 0.8395 time: 182.93435883522034\n",
            "epoch: 16 lr: 0.1394375 train loss: 0.22965875 train acc: 0.92186 val loss: 0.43638125 val acc: 0.8553 time: 194.8295657634735\n",
            "epoch: 17 lr: 0.11575000000000002 train loss: 0.204079375 train acc: 0.93002 val loss: 0.3266 val acc: 0.8911 time: 206.72693610191345\n",
            "epoch: 18 lr: 0.09206250000000005 train loss: 0.175188125 train acc: 0.94218 val loss: 0.318125 val acc: 0.8932 time: 218.44927191734314\n",
            "epoch: 19 lr: 0.06837500000000002 train loss: 0.1478375 train acc: 0.95122 val loss: 0.25701875 val acc: 0.9143 time: 230.1474552154541\n",
            "epoch: 20 lr: 0.04468749999999999 train loss: 0.118089375 train acc: 0.96166 val loss: 0.29724375 val acc: 0.9039 time: 242.1692807674408\n",
            "epoch: 21 lr: 0.021 train loss: 0.0966659375 train acc: 0.97032 val loss: 0.199134375 val acc: 0.9346 time: 254.6014904975891\n",
            "epoch: 22 lr: 0.014000000000000002 train loss: 0.0771359375 train acc: 0.97674 val loss: 0.1883 val acc: 0.9396 time: 267.16229701042175\n",
            "epoch: 23 lr: 0.007000000000000001 train loss: 0.071721875 train acc: 0.97934 val loss: 0.185525 val acc: 0.94 time: 279.6129050254822\n",
            "epoch: 24 lr: 0.0 train loss: 0.0656475 train acc: 0.98078 val loss: 0.1839625 val acc: 0.9408 time: 292.1171534061432\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}